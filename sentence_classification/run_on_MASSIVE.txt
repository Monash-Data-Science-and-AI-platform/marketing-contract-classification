To run this model on MASSIVE, there are some steps needed to be done

Documentation of MASSIVE is available at https://docs.massive.org.au/
___________________________________________________________________________________________________________________________________________________________
Creating miniconda:

1. login to MASSIVE via https://beta.desktop.cvl.org.au/login, choose CVL
2. Click terminal and select single T4 gpu(own preference) and then launch
3. Under pending/running terminal, click connect
4. setup miniconda environment, documentation is available at https://docs.massive.org.au/M3/software/pythonandconda/python-miniconda.html?highlight=miniconda

___________________________________________________________________________________________________________________________________________________________
To upload large amount of file:

Use Filezilla to transfer files(especially the dataset) into MASSIVE, documentation is available at: https://docs.massive.org.au/M3/transferring-files.html


___________________________________________________________________________________________________________________________________________________________
To obatain code from GitHub repository:
1. Clone the repository by running this command in terminal
$ git clone https://github.com/Monash-Data-Science-and-AI-platform/marketing-contract-classification

2. You might need your GitHub username and access token(enter access token whenever password is asked for GitHub in terminal)
- To setup access token, documentation is available at: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token
- make sure the access token is copied and save somewhere else as it is not visible anymore once you leaved the page

3. whenever there are changes made, commited and pushed to the repository from another computer, we need to pull the changes:
-Run the following command
$ cd [path to local repository]
$ git pull

-you also need your GitHub username and access token after these two lines of command
___________________________________________________________________________________________________________________________________________________________
Update code to GitHub repository
1. in terminal, run the command:
$ cd [path to local repository]

2. run the command:
$ git add [path to the file modified]       (without the local repository path)

3. add all the modified files by running step 2

4. run the command:
$ git commit

and , then press enter

5. write the commit message:
- press esc , then i
-write the command at the blank space on top
-then press esc, then :wq

6. run the command:
-$ git push
-type in your GitHub username and then your GitHub personal access token
_______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
Alternative:
1. Install VScode via https://code.visualstudio.com/download
2. Follow the documentation on how to install and run python on VSCode on https://code.visualstudio.com/docs/languages/python
3. Install GitHub desktop via https://desktop.github.com/
4. Setup GitHub on VScode , documentation is available at https://adamtheautomator.com/visual-studio-code-github-setup/
5. Clone the remote repository , documentation is available on https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/adding-and-cloning-repositories/cloning-and-forking-repositories-from-github-desktop
6. Each time when changes are made on local computer, make sure you write the commit message, commit and push to remote repository
Documentation is avaiable on https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/making-changes-in-a-branch/committing-and-reviewing-changes-to-your-project

7. Pulling process is still the same via MASSIVE
_________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
Installation of modules:

After setting up miniconda , and the taskbar on the left, Jupyter Lab from .scratch.ec18.[username].miniconda should be available
1. Launch the  Jupyter Lab from [].miniconda terminal
2. Launch the terminal in the launcher(inside Jupyter Lab from [].miniconda terminal)
3. Activate the conda environment with the command:  source [path to miniconda/bin/activate]
4. Create the virtual environment with the command: conda env update -n base --file [path to environment.yml]

__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
Login to wandb in terminal: 
-make sure you created a wandb account via: https://app.wandb.ai/login?signup=true

1. activate miniconda environment by running the command:
$ source [path to miniconda/bin/activate]

2. run the command:
$ wandb login

3.You should need your wandb username and api key(available in your wandb account)


___________________________________________________________________________________________________________________________________________________________
Running py code via submitting slurm job script:

-open the terminal and run the command:

sbatch slurm-gpu-job-script

-more info in: example slurm-gpu-job-script.txt and slurm-gpu-job-script guide.txt

___________________________________________________________________________________________________________________________________________________________
Important notes for submitting multiple job at a time on MASSIVE
1. check the .out file specified in slurm job script, make sure 'training started' is printed in the file before Pulling
new codes from GitHub or make any changes to local codes 

--> This applies to both fine_tuning.py, pre_training.py and inference.py

___________________________________________________________________________________________________________________________________________________________

Common command used in MASSIVE terminal:


show_cluster //show the status of all CPU and GPU on MASSIVE

source [path to miniconda]/miniconda/bin/activate //activate conda environment

conda list //check modules downloaded in conda

ls //check job

show_job // show all jobs being submitted

cd //go to certain directories

cd -   //go back to previous directory

git pull //pull changes from remote repository

sbatch slurm-gpu-job-script //submit slurm job script

ctrl+c //force stop 
________________________________________________________________________________________________________________________________________________________
M3 MASSIVE speicfic issues:
1. TypeError: Invalid keyword argument(s) in `compile`: {'steps_per_execution'}
-This is due to the default tensorflow-gpu 2.2.0 does not support {'steps_per_execution'} in model.fit()
-Solution:
	-download tensorflow-gpu 2.4.0 or later version of tensorflow
	-in the slurm-gpu-job-script:
		-module unload cuda
		-module load cuda/11.0 
	#tensorflow-gpu 2.4.0 requires cuda 11.0, if other version of tensorflow is used, please refer to https://www.tensorflow.org/install/source#tested_build_configurations
	#to get the correct cuda version

2. Weird [Errno 2] No such file or directory or ModuleNotFoundError (even though module is installed in miniconda)
-Solution: Do not define module load tensorflow in slurm-gpu-job-script

3. unable to read .xlsx fie
-Solutions: 
	-install openpyxl in miniconda
	-In the .py file, define:  import openpyxl
	-in the line for pd.read_excel, define the argument in pd.read_excel, engine='openpyxl'

4. Resource exhaust error
-For V100 GPU, pre-training only allow up until batch size of 16(for pre-training)


5. https://huggingface.co/nlpaueb/bert-base-uncased-contracts(or similar url) takes too long to response
-Solutions: wait for a while a resubmit the job

6. The command /usr/local/sv2/dev/jupyter/jupyter_params.py {jobid} on host m3t102 timed out. Is the application server OK?
   Attempting to create an SSH tunnel tom3t101 via m3.massive.org.au unexpectedly returned an error message. Please report this via the contact us link. The error message was Unable to create the secure tunnel. Please try again.
-Solutions: 	-wait for a while after launching the jupyter lab, and then click connect
		-if error message pops-up, click connect again, repeat until you connected to the jupyter lab

-reasons: jupyter process is very slow to startup. The job begins but it takes some time before the server is actually running
